{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import get to call a get request on the site\n",
    "from requests import get\n",
    "\n",
    "#get the first page of the east bay housing prices\n",
    "response = get('https://sfbay.craigslist.org/search/hhh?s=120&availabilityMode=0&max_bedrooms=2&max_price=4500&min_bedrooms=2&postal=94117&search_distance=1.5') #get rid of those lame-o's that post a housing option without a pic using their filter\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "#get the macro-container for the housing posts\n",
    "posts = html_soup.find_all('li', class_= 'result-row')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build out the loop\n",
    "from time import sleep\n",
    "import re\n",
    "from random import randint #avoid throttling by not sending too many requests one after the other\n",
    "from warnings import warn\n",
    "from time import time\n",
    "from IPython.core.display import clear_output\n",
    "import numpy as np\n",
    "\n",
    "#find the total number of posts to find the limit of the pagination\n",
    "results_num = html_soup.find('div', class_= 'search-legend')\n",
    "results_total = int(results_num.find('span', class_='totalcount').text) #pulled the total count of posts as the upper bound of the pages array\n",
    "\n",
    "#each page has 119 posts so each new page is defined as follows: s=120, s=240, s=360, and so on. So we need to step in size 120 in the np.arange function\n",
    "pages = np.arange(0, results_total+1, 120)\n",
    "\n",
    "iterations = 0\n",
    "\n",
    "post_timing = []\n",
    "post_hoods = []\n",
    "post_title_texts = []\n",
    "bedroom_counts = []\n",
    "sqfts = []\n",
    "post_links = []\n",
    "post_prices = []\n",
    "\n",
    "for page in pages:\n",
    "    print(\"page \" + str(page) )\n",
    "    #get request\n",
    "    response = get(\"https://sfbay.craigslist.org/search/hhh?\" \n",
    "                   + \"s=\" #the parameter for defining the page number \n",
    "                   + str(page) #the page number in the pages array from earlier\n",
    "                   + \"&availabilityMode=0&max_bedrooms=2&max_price=4500&min_bedrooms=2&postal=94117&search_distance=1.5\")\n",
    "\n",
    "    sleep(randint(1,5))\n",
    "     \n",
    "    #throw warning for status codes that are not 200\n",
    "    if response.status_code != 200:\n",
    "        warn('Request: {}; Status code: {}'.format(requests, response.status_code))\n",
    "        \n",
    "    #define the html text\n",
    "    page_html = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    #define the posts\n",
    "    posts = html_soup.find_all('li', class_= 'result-row')\n",
    "        \n",
    "    #extract data item-wise\n",
    "    for post in posts:\n",
    "\n",
    "        if post.find('span', class_ = 'result-hood') is not None:\n",
    "\n",
    "            #posting date\n",
    "            #grab the datetime element 0 for date and 1 for time\n",
    "            post_datetime = post.find('time', class_= 'result-date')['datetime']\n",
    "            post_timing.append(post_datetime)\n",
    "\n",
    "            #neighborhoods\n",
    "            post_hood = post.find('span', class_= 'result-hood').text\n",
    "            post_hoods.append(post_hood)\n",
    "\n",
    "            #title text\n",
    "            post_title = post.find('a', class_='result-title hdrlnk')\n",
    "            post_title_text = post_title.text\n",
    "            post_title_texts.append(post_title_text)\n",
    "\n",
    "            #post link\n",
    "            post_link = post_title['href']\n",
    "            post_links.append(post_link)\n",
    "            \n",
    "            #removes the \\n whitespace from each side, removes the currency symbol, and turns it into an int\n",
    "            try:\n",
    "                post_price = int(post.a.text.strip().replace(\"$\", \"\")) \n",
    "                post_prices.append(post_price)\n",
    "            except ValueError as e:\n",
    "                print(\"error\",e)\n",
    "                post_prices.append(0)\n",
    "            \n",
    "            if post.find('span', class_ = 'housing') is not None:\n",
    "                \n",
    "                #if the first element is accidentally square footage\n",
    "                if 'ft2' in post.find('span', class_ = 'housing').text.split()[0]:\n",
    "                    \n",
    "                    #make bedroom nan\n",
    "                    bedroom_count = np.nan\n",
    "                    bedroom_counts.append(bedroom_count)\n",
    "                    \n",
    "                    #make sqft the first element\n",
    "                    sqft = int(post.find('span', class_ = 'housing').text.split()[0][:-3])\n",
    "                    sqfts.append(sqft)\n",
    "                    \n",
    "                #if the length of the housing details element is more than 2\n",
    "                elif len(post.find('span', class_ = 'housing').text.split()) > 2:\n",
    "                    \n",
    "                    #therefore element 0 will be bedroom count\n",
    "                    bedroom_count = post.find('span', class_ = 'housing').text.replace(\"br\", \"\").split()[0]\n",
    "                    bedroom_counts.append(bedroom_count)\n",
    "                    \n",
    "                    #and sqft will be number 3, so set these here and append\n",
    "                    sqft = int(post.find('span', class_ = 'housing').text.split()[2][:-3])\n",
    "                    sqfts.append(sqft)\n",
    "                    \n",
    "                #if there is num bedrooms but no sqft\n",
    "                elif len(post.find('span', class_ = 'housing').text.split()) == 2:\n",
    "                    \n",
    "                    #therefore element 0 will be bedroom count\n",
    "                    bedroom_count = post.find('span', class_ = 'housing').text.replace(\"br\", \"\").split()[0]\n",
    "                    bedroom_counts.append(bedroom_count)\n",
    "                    \n",
    "                    #and sqft will be number 3, so set these here and append\n",
    "                    sqft = np.nan\n",
    "                    sqfts.append(sqft)                    \n",
    "                \n",
    "                else:\n",
    "                    bedroom_count = np.nan\n",
    "                    bedroom_counts.append(bedroom_count)\n",
    "                \n",
    "                    sqft = np.nan\n",
    "                    sqfts.append(sqft)\n",
    "                \n",
    "            #if none of those conditions catch, make bedroom nan, this won't be needed    \n",
    "            else:\n",
    "                bedroom_count = np.nan\n",
    "                bedroom_counts.append(bedroom_count)\n",
    "                \n",
    "                sqft = np.nan\n",
    "                sqfts.append(sqft)\n",
    "            #    bedroom_counts.append(bedroom_count)\n",
    "                \n",
    "            #    sqft = np.nan\n",
    "            #    sqfts.append(sqft)\n",
    "                \n",
    "    iterations += 1\n",
    "    print(\"Page \" + str(iterations) + \" scraped successfully!\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Scrape complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "eb_apts = pd.DataFrame({'posted': post_timing,\n",
    "                       'neighborhood': post_hoods,\n",
    "                       'post title': post_title_texts,\n",
    "                       'number bedrooms': bedroom_counts,\n",
    "                        'sqft': sqfts,\n",
    "                        'URL': post_links,\n",
    "                       'price': post_prices})\n",
    "print(eb_apts.info())\n",
    "eb_apts.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first things first, drop duplicate URLs because people are spammy on Craigslist. \n",
    "#Let's see how many uniqe posts we really have.\n",
    "eb_apts = eb_apts.drop_duplicates(subset='URL')\n",
    "len(eb_apts.drop_duplicates(subset='URL'))\n",
    "\n",
    "#make the number bedrooms to a float (since np.nan is a float too)\n",
    "eb_apts['number bedrooms'] = eb_apts['number bedrooms'].apply(lambda x: float(x))\n",
    "\n",
    "#convert datetime string into datetime object to be able to work with it\n",
    "from datetime import datetime\n",
    "\n",
    "eb_apts['posted'] = pd.to_datetime(eb_apts['posted'])\n",
    "\n",
    "#Looking at what neighborhoods there are with eb_apts['neighborhood'].unique() allowed me to see what\n",
    "#I needed to deal with in terms of cleaning those.\n",
    "\n",
    "#remove the parenthesis from the left and right of the neighborhoods\n",
    "eb_apts['neighborhood'] = eb_apts['neighborhood'].map(lambda x: x.lstrip('(').rstrip(')'))\n",
    "\n",
    "#titlecase them\n",
    "eb_apts['neighborhood'] = eb_apts['neighborhood'].str.title()\n",
    "\n",
    "#just take the first name of the neighborhood list, splitting on the '/' delimiter\n",
    "eb_apts['neighborhood'] = eb_apts['neighborhood'].apply(lambda x: x.split('/')[0])\n",
    "\n",
    "#fix one-offs that\n",
    "eb_apts['neighborhood'].replace('Belmont, Ca', 'Belmont', inplace=True)\n",
    "eb_apts['neighborhood'].replace('Hercules, Pinole, San Pablo, El Sob', 'Hercules', inplace=True)\n",
    "\n",
    "#remove whitespaces\n",
    "eb_apts['neighborhood'] = eb_apts['neighborhood'].apply(lambda x: x.strip())\n",
    "\n",
    "#save the clean data\n",
    "eb_apts.to_csv(\"eb_apts_1642_Jan_2_19_clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as pylab\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "params = {'legend.fontsize': 'x-large',\n",
    "          'figure.figsize': (15, 5),\n",
    "         'axes.labelsize': 'x-large',\n",
    "         'axes.titlesize':'x-large',\n",
    "         'xtick.labelsize':'x-large',\n",
    "         'ytick.labelsize':'x-large'}\n",
    "pylab.rcParams.update(params)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(x='price', y='sqft', hue='number bedrooms', palette='summer', x_jitter=True, y_jitter=True, s=125, data=eb_apts.dropna())\n",
    "plt.legend(fontsize=12)\n",
    "plt.xlabel(\"Price\", fontsize=18)\n",
    "plt.ylabel(\"Square Footage\", fontsize=18);\n",
    "plt.title(\"Price vs. Square Footage Colored by Number of Bedrooms\", fontsize=18);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "sns.regplot(x='price', y='sqft', data=eb_apts.dropna());\n",
    "plt.title('Price vs. Square Footage Regression Plot');\n",
    "plt.xlabel(\"Price (USD)\");\n",
    "plt.ylabel(\"Square Feet\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='neighborhood', y='price', data=eb_apts)\n",
    "plt.xlabel(\"Neighborhood\");\n",
    "plt.xticks(rotation=75)\n",
    "plt.ylabel(\"Price USD\");\n",
    "plt.title(\"Prices by Neighborhood - Boxplots\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
